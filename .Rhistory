removed <- anti_join(validation, data_sub)
data_sub <- rbind(data_sub, removed)
# Remove unneeded objects
rm(val_index, removed)
# Split data into training and test sets
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = data_sub$Total.Cup.Points, times = 1, p = 0.1, list = FALSE)
train_set <- data_sub[-test_index,]
test_set <- data_sub[test_index,]
#Add the rows removed from the test_set back into train_set and remove unneeded objects
removed <- anti_join(test_set, data_sub)
train_set <- rbind(train_set, removed)
# Remove unneeded objects
rm(test_index, removed)
# Create the function for calculating Root Mean Squared Error (RMSE)
RMSE <- function(true_ratings, predicted_ratings){
sqrt(mean((true_ratings - predicted_ratings) ^ 2))
}
#######################################################################
#   Exploratory Analyses
#######################################################################
# Count distinct values for each column
col_index <- c(1:ncol(data_sub))
n_dist <- function(column){
n_distinct(data_sub[,column])
}
num_dist_values <- sapply(col_index, n_dist)
# Overall average of Total.Cup.Points
mu_train <- mean(train_set$Total.Cup.Points)
mu_train
##########################
# Naive Model
##########################
# Rating difference between overall average of the train_set and true ratings of test_set
naive_rmse <- RMSE(test_set$Total.Cup.Points, mu_train)
rmse_results <- tibble(Method = "Naive Model", RMSE = naive_rmse)
#############################
# Country of Origin Effects
#############################
# country-specific effect
country_effects <- train_set %>%
group_by(Country.of.Origin) %>%
summarize(b_c = mean(Total.Cup.Points - mu_train))
# 1) predictions
predicted_ratings_1 <- test_set %>%
left_join(country_effects, by = 'X', na_matches = "never") %>%
mutate(predictions = mu_train + b_m) %>%
pull(predictions)
country_effects
#############################
# Country of Origin Effects
#############################
# country-specific effect
country_effects <- train_set %>%
group_by(Country.of.Origin) %>%
summarize(b_c = mean(Total.Cup.Points - mu_train))
# 1) predictions
predicted_ratings_1 <- test_set %>%
left_join(country_effects, by = 'Country.of.Origin', na_matches = "never") %>%
mutate(predictions = mu_train + b_m) %>%
pull(predictions)
# 1) predictions
predicted_ratings_1 <- test_set %>%
left_join(country_effects, by = 'Country.of.Origin', na_matches = "never") %>%
mutate(predictions = mu_train + b_c) %>%
pull(predictions)
# check for missing values
sum(is.na(movie_effects))
# check for missing values
sum(is.na(country_effects))
sum(is.na(predicted_ratings_1))
predicted_ratings_1[is.na(predicted_ratings_1)==1]
# Chose to replace them with the overall mean of 3.513
predicted_ratings_c1 <- ifelse(is.na(predicted_ratings_1), br, predicted_ratings_1)
# results
country_rmse <- RMSE(test_set$Total.Cup.Points, predicted_ratings_c1)
rmse_results <- rmse_results %>% add_row(Method = "Country Only", RMSE = country_rmse)
rmse_results
?prcomp
fit <- train_set %>% lm(Total.Cup.Points ~ Country.of.Origin, Region, Farm.Name,
Producer, Variety, Processing.Method, Category.One.Defects,
Category.Two.Defects, altitude_mean_meters)
fit <- train_set %>% lm(Total.Cup.Points ~ Country.of.Origin + Region + Farm.Name +
Producer + Variety + Processing.Method + Category.One.Defects +
Category.Two.Defects + altitude_mean_meters)
fit <- train_set %>% lm(Total.Cup.Points ~ Country.of.Origin)
fit <- train_set %>% lm(Total.Cup.Points ~ Country.of.Origin, na.action = na.omit())
fit <- lm(Total.Cup.Points ~ Country.of.Origin, data = train_set, na.action = na.omit())
fit <- lm(Total.Cup.Points ~ Country.of.Origin, data = train_set, na.action = na.omit)
fit
fit <- lm(Total.Cup.Points ~ Country.of.Origin + Region, data = train_set, na.action = na.omit)
fit
fit <- lm(Total.Cup.Points ~ Country.of.Origin + Category.One.Defects, data = train_set, na.action = na.omit)
fit
summary(fit)
View(train_set)
View(train_set)
# Measure completeness of data in each column
col_complete <- function(column){
(nrow(column) - sum(is.na(column))) / nrow(column)
}
column_integrity <- sapply(col_index, col_complete)
column_integrity
col_index
(nrow(data_sub$Country.of.Origin) - sum(is.na(data_sub$Country.of.Origin))) / nrow(data_sub$Country.of.Origin)
nrow(data_sub$Country.of.Origin)
(nrow(data_sub) - sum(is.na(data_sub$Country.of.Origin))) / nrow(data_sub)
dataset <- data_sub
col_complete <- function(dataset, column){
(nrow(dataset) - sum(is.na(column))) / nrow(dataset)
}
column_integrity <- sapply(col_index, col_complete)
col_complete <- function(dataset, column){
(nrow(dataset) - sum(is.na(dataset[,column]))) / nrow(dataset)
}
column_integrity <- sapply(col_index, col_complete(dataset = data_sub, ))
# Measure completeness of data in each column
col_complete <- function(dataset, column){
(nrow(dataset) - sum(is.na(dataset[,column]))) / nrow(dataset)
}
column_integrity <- sapply(col_index, col_complete)
# Measure completeness of data in each column
col_complete <- function(dataset, column){
dataset <- data_sub
(nrow(dataset) - sum(is.na(dataset[,column]))) / nrow(dataset)
}
column_integrity <- sapply(col_index, col_complete)
column_integrity
(nrow(data_sub) - sum(is.na(data_sub[,1]))) / nrow(data_sub)
sum(is.na(data_sub[,1]))
(nrow(data_sub) - sum(is.na(data_sub[,2]))) / nrow(data_sub)
(nrow(data_sub) - sum(is.na(data_sub[,3]))) / nrow(data_sub)
(nrow(data_sub) - sum(is.na(data_sub[,30]))) / nrow(data_sub)
(nrow(data_sub) - sum(is.na(data_sub[,31]))) / nrow(data_sub)
source("~/Projects/harvardx-capstone/harvardx_capstone_personal_coffee_trh.R", echo=TRUE)
##########################################################
#   HarvardX PH125.9x - Data Science: Capstone
#     Personal Project Submission
#       Travis Horesh
#         November 2022
##########################################################
#######################################################################
# Read in raw dataset from downloaded txt file
#######################################################################
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridaet", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(httr)) install.packages("httr", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(lubridate)
library(data.table)
library(httr)
# Arabica Coffee Grading Dataset by Diego Volpatto via Kaggle:
# https://www.kaggle.com/datasets/volpatto/coffee-quality-database-from-cqi?select=arabica_data_cleaned.csv
data_raw <- read.csv(
file = "/Users/GreenTea/Projects/harvardx-capstone/arabica_data_cleaned.txt",
na.strings = c("", " ")
)
data <- as_tibble(data_raw)
glimpse(data)
head(data)
#######################################################################
#   Data Cleaning and Transformation
#######################################################################
# remove unneeded columns
data <- data %>% select(-Species, -ICO.Number, -Altitude, -Number.of.Bags, -Bag.Weight,
-In.Country.Partner, -Expiration, -Certification.Body, -Certification.Contact,
-Certification.Address)
head(data)
# convert text dates to actual dates
data <- data %>%
mutate(
Harvest.Year = as.numeric(str_sub(data$Harvest.Year, start = -5)),
Grading.Date = mdy(Grading.Date)
)
head(data)
# convert altitude values to numbers
data <- data %>%
mutate(
altitude_low_meters = ifelse(is.na(altitude_low_meters), altitude_low_meters, as.numeric(altitude_low_meters)),
altitude_high_meters = ifelse(is.na(altitude_high_meters), altitude_high_meters, as.numeric(altitude_high_meters)),
altitude_mean_meters = ifelse(is.na(altitude_mean_meters), altitude_mean_meters, as.numeric(altitude_mean_meters))
)
head(data[,"altitude_low_meters"])
# convert to meters if unit_of_measurement is in feet
conv_coeff <- 0.3048    # meters per foot
replace_index <- data$unit_of_measurement == "ft"
data$altitude_low_meters[replace_index] <- (ifelse(is.na(data$altitude_low_meters[replace_index]),
data$altitude_low_meters[replace_index],
data$altitude_low_meters[replace_index] / conv_coeff                                                   ))
data$altitude_high_meters[replace_index] <- (ifelse(is.na(data$altitude_high_meters[replace_index]),
data$altitude_high_meters[replace_index],
data$altitude_high_meters[replace_index] / conv_coeff))
data$altitude_mean_meters[replace_index] <- (ifelse(is.na(data$altitude_mean_meters[replace_index]),
data$altitude_mean_meters[replace_index],
data$altitude_mean_meters[replace_index] / conv_coeff))
#######################################################################
#   Create validation, train, and test datasets
#######################################################################
# Validation set will be 10% of the data
set.seed(1, sample.kind="Rounding")
val_index <- createDataPartition(y = data$Total.Cup.Points, times = 1, p = 0.1, list = FALSE)
data_sub <- data[-val_index,]
validation <- data[val_index,]
# Add rows removed from validation set back into main dataset
removed <- anti_join(validation, data_sub)
data_sub <- rbind(data_sub, removed)
# Remove unneeded objects
rm(val_index, removed)
# Split data into training and test sets
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = data_sub$Total.Cup.Points, times = 1, p = 0.1, list = FALSE)
train_set <- data_sub[-test_index,]
test_set <- data_sub[test_index,]
#Add the rows removed from the test_set back into train_set and remove unneeded objects
removed <- anti_join(test_set, data_sub)
train_set <- rbind(train_set, removed)
# Remove unneeded objects
rm(test_index, removed)
# Create the function for calculating Root Mean Squared Error (RMSE)
RMSE <- function(true_ratings, predicted_ratings){
sqrt(mean((true_ratings - predicted_ratings) ^ 2))
}
#######################################################################
#   Exploratory Analyses
#######################################################################
# Count distinct values for each column
col_index <- c(1:ncol(data_sub))
n_dist <- function(column){
n_distinct(data_sub[,column])
}
num_dist_values <- sapply(col_index, n_dist)
num_dist_values
# Measure completeness of data in each column
col_complete <- function(dataset, column){
dataset <- data_sub
(nrow(dataset) - sum(is.na(dataset[,column]))) / nrow(dataset)
}
column_integrity <- sapply(col_index, col_complete)
column_integrity
(nrow(data_sub) - sum(is.na(data_sub[,12]))) / nrow(data_sub)
# Overall average of Total.Cup.Points
mu_train <- mean(train_set$Total.Cup.Points)
mu_train
# --------------------------------------------------------------------- #
#   Algorithm development using edx dataset
# ----------------------------------------------------------------------#
##########################
# Naive Model
##########################
# Rating difference between overall average of the train_set and true ratings of test_set
naive_rmse <- RMSE(test_set$Total.Cup.Points, mu_train)
rmse_results <- tibble(Method = "Naive Model", RMSE = naive_rmse)
#############################
# Country of Origin Effects
#############################
# country-specific effect
country_effects <- train_set %>%
group_by(Country.of.Origin) %>%
summarize(b_c = mean(Total.Cup.Points - mu_train))
# 1) predictions
predicted_ratings_1 <- test_set %>%
left_join(country_effects, by = 'Country.of.Origin', na_matches = "never") %>%
mutate(predictions = mu_train + b_c) %>%
pull(predictions)
# check for missing values
sum(is.na(country_effects))
sum(is.na(predicted_ratings_1))
predicted_ratings_1[is.na(predicted_ratings_1)==1]
# Chose to replace them with the overall mean of 3.513
predicted_ratings_c1 <- ifelse(is.na(predicted_ratings_1), br, predicted_ratings_1)
# results
country_rmse <- RMSE(test_set$Total.Cup.Points, predicted_ratings_c1)
rmse_results <- rmse_results %>% add_row(Method = "Country Only", RMSE = country_rmse)
#############################
# lm function
#############################
fit <- lm(Total.Cup.Points ~ Country.of.Origin + Category.One.Defects, data = train_set, na.action = na.omit)
summary(fit)
(nrow(data_sub) - sum(is.na(data_sub[,12]))) / nrow(data_sub)
(nrow(data_sub) - sum(is.na(data_sub[,34]))) / nrow(data_sub)
# Measure completeness of data in each column
col_complete <- function(column){
(nrow(data_sub) - sum(is.na(data_sub[,column]))) / nrow(data_sub)
}
column_integrity <- sapply(col_index, col_complete)
column_integrity
rbind(colnames(data_sub), column_integrity)
?as.matrix
as.matrix(column_integrity, dimnames(colnames(data_sub)))
as.matrix(column_integrity, byrow = TRUE, dimnames(colnames(data_sub)))
as.matrix(column_integrity, byrow = TRUE)
z <- as.matrix(column_integrity, nrow = 1, ncol = ncol(column_integrity), byrow = TRUE)
colnames(z) <- colnames(data_sub)
z <- as.matrix(column_integrity, nrow = 1, ncol = ncol(data_sub), byrow = TRUE)
colnames(z) <- colnames(data_sub)
ncol(data_sub)
z <- as.matrix(column_integrity, nrow = 1, ncol = ncol(data_sub), byrow = TRUE)
colnames(z)
colnames(data_sub)
l <- as.list(colnames(data_sub))
colnames(z) <- l
l <- as.list(colnames(data_sub))
l
l <- as.vector(colnames(data_sub))
l
colnames(z) <- l
colnames(z)
column_integrity
rbind(column_integrity, colnames(data_sub))
fit <- lm(Total.Cup.Points ~ Country.of.Origin + Region + Harvest.Year + Variety +
Processing.Method + Category.One.Defects + Quakers + Category.Two.Defects +
altitude_mean_meters, data = train_set, na.action = na.omit)
summary(fit)
predict(fit, test_set)
?prcomp
class(num_dist_values)
str(num_dist_values)
str(as.vector(num_dist_values))
class(as.vector(num_dist_values))
as_tibble(num_dist_values)
?as_tibble
ndv <- matrix(num_dist_values, ncol = ncol(num_dist_values))
ndv <- matrix(num_dist_values, ncol = 34)
colnames(ndv) <- colnames(data_sub)
df <- as_tibble(ndv)
df
ndv <- matrix(num_dist_values, ncol = ncol(data_sub))
colnames(ndv) <- colnames(data_sub)
df <- as_tibble(ndv)
df
num_dist_values <- matrix(num_dist_values, ncol = ncol(data_sub))
colnames(num_dist_values) <- colnames(data_sub)
num_dist_values <- as_tibble(num_dist_values)
num_dist_values
column_integrity <- matrix(column_integrity, ncol = ncol(data_sub))
colnames(column_integrity) <- colnames(data_sub)
column_integrity <- as_tibble(column_integrity)
column_integrity
column_integrity[colSums(column_integrity) >= 0.7]
column_integrity[colSums(column_integrity) >= 0.9]
data_sub %>%
ggplot(aes(Total.Cup.Points, Cupper.Points)) +
geom_point(alpha = 0.5)
# scatterplot of Total Cup Points vs. Category One Defects
data_sub %>%
ggplot(aes(Total.Cup.Points, Category.One.Defects)) +
geom_point(alpha = 0.5)
# scatterplot of Total Cup Points vs. Category Two Defects
data_sub %>%
ggplot(aes(Total.Cup.Points, Category.Two.Defects)) +
geom_point(alpha = 0.5)
# scatterplot of Total Cup Points vs. Quakers
data_sub %>%
ggplot(aes(Total.Cup.Points, Quakers)) +
geom_point(alpha = 0.5)
# scatterplot of Total Cup Points vs. Mean Elevation
data_sub %>%
ggplot(aes(Total.Cup.Points, altitude_mean_meters)) +
geom_point(alpha = 0.5)
# box plot of Total Cup Points vs. Category One Defects
data_sub %>%
ggplot(aes(Total.Cup.Points, Category.One.Defects)) +
geom_boxplot()
data_sub %>%
ggplot(aes(Total.Cup.Points, Category.One.Defects, group = Category.One.Defects)) +
geom_boxplot()
?aes
data_sub[is.na(data_sub$altitude_mean_meters)==0] %>%
ggplot(aes(Total.Cup.Points, altitude_mean_meters)) +
geom_point(alpha = 0.5)
data_sub[is.na(data_sub$altitude_mean_meters)==0,] %>%
ggplot(aes(Total.Cup.Points, altitude_mean_meters)) +
geom_point(alpha = 0.5)
data_sub[is.na(data_sub$altitude_mean_meters)==0 & data_sub$altitude_mean_meters < 5000,] %>%
ggplot(aes(Total.Cup.Points, altitude_mean_meters)) +
geom_point(alpha = 0.5)
data_raw <- read.csv(
file = "/Users/GreenTea/Projects/harvardx-capstone/arabica_data_cleaned.txt",
na.strings = c("", " ")
)
data <- as_tibble(data_raw)
glimpse(data)
head(data)
#######################################################################
#   Data Cleaning and Transformation
#######################################################################
# remove unneeded columns
data <- data %>% select(-Species, -ICO.Number, -Altitude, -Number.of.Bags, -Bag.Weight,
-In.Country.Partner, -Expiration, -Certification.Body, -Certification.Contact,
-Certification.Address)
head(data)
# convert text dates to actual dates
data <- data %>%
mutate(
Harvest.Year = as.numeric(str_sub(data$Harvest.Year, start = -5)),
Grading.Date = mdy(Grading.Date)
)
head(data)
# convert altitude values to numbers
data <- data %>%
mutate(
altitude_low_meters = ifelse(is.na(altitude_low_meters), altitude_low_meters, as.numeric(altitude_low_meters)),
altitude_high_meters = ifelse(is.na(altitude_high_meters), altitude_high_meters, as.numeric(altitude_high_meters)),
altitude_mean_meters = ifelse(is.na(altitude_mean_meters), altitude_mean_meters, as.numeric(altitude_mean_meters))
)
head(data[,"altitude_low_meters"])
# convert altitude values to meters if unit_of_measurement is in feet
conv_coeff <- 0.3048    # meters per foot
replace_index <- data$unit_of_measurement == "ft"
data$altitude_low_meters[replace_index] <- (ifelse(is.na(data$altitude_low_meters[replace_index]),
data$altitude_low_meters[replace_index],
data$altitude_low_meters[replace_index] * conv_coeff                                                   ))
data$altitude_high_meters[replace_index] <- (ifelse(is.na(data$altitude_high_meters[replace_index]),
data$altitude_high_meters[replace_index],
data$altitude_high_meters[replace_index] * conv_coeff))
data$altitude_mean_meters[replace_index] <- (ifelse(is.na(data$altitude_mean_meters[replace_index]),
data$altitude_mean_meters[replace_index],
data$altitude_mean_meters[replace_index] * conv_coeff))
#######################################################################
#   Create validation, train, and test datasets
#######################################################################
# Validation set will be 10% of the data
set.seed(1, sample.kind="Rounding")
val_index <- createDataPartition(y = data$Total.Cup.Points, times = 1, p = 0.1, list = FALSE)
data_sub <- data[-val_index,]
validation <- data[val_index,]
# #Confirm userId and movieId are in both the train and test sets
# validation <- val_temp %>%
#   semi_join(data_sub, by = "X") %>%
#   semi_join(data_sub, by = "Owner") %>%
#   semi_join(data_sub, by = "Country.of.Origin") %>%
#   semi_join(data_sub, by = Region)
# Add rows removed from validation set back into main dataset
removed <- anti_join(validation, data_sub)
data_sub <- rbind(data_sub, removed)
# Remove unneeded objects
rm(val_index, removed)
# Split data into training and test sets
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = data_sub$Total.Cup.Points, times = 1, p = 0.1, list = FALSE)
train_set <- data_sub[-test_index,]
test_set <- data_sub[test_index,]
#Add the rows removed from the test_set back into train_set and remove unneeded objects
removed <- anti_join(test_set, data_sub)
train_set <- rbind(train_set, removed)
# Remove unneeded objects
rm(test_index, removed)
# Create the function for calculating Root Mean Squared Error (RMSE)
RMSE <- function(true_ratings, predicted_ratings){
sqrt(mean((true_ratings - predicted_ratings) ^ 2))
}
#######################################################################
#   Exploratory Analyses
#######################################################################
# Count distinct values for each column
col_index <- c(1:ncol(data_sub))
n_dist <- function(column){
n_distinct(data_sub[,column])
}
num_dist_values <- sapply(col_index, n_dist)
num_dist_values <- matrix(num_dist_values, ncol = ncol(data_sub))
colnames(num_dist_values) <- colnames(data_sub)
num_dist_values <- as_tibble(num_dist_values)
# Measure completeness of data in each column
col_complete <- function(column){
(nrow(data_sub) - sum(is.na(data_sub[,column]))) / nrow(data_sub)
}
column_integrity <- sapply(col_index, col_complete)
column_integrity <- matrix(column_integrity, ncol = ncol(data_sub))
colnames(column_integrity) <- colnames(data_sub)
column_integrity <- as_tibble(column_integrity)
column_integrity[colSums(column_integrity) >= 0.9]
# Overall average of Total.Cup.Points
mu_train <- mean(train_set$Total.Cup.Points)
mu_train
# scatter plot of Total Cup Points vs. Cupper Points
data_sub %>%
ggplot(aes(Total.Cup.Points, Cupper.Points)) +
geom_point(alpha = 0.5)
# box plot of Total Cup Points vs. Category One Defects
data_sub %>%
ggplot(aes(Total.Cup.Points, Category.One.Defects, group = Category.One.Defects)) +
geom_boxplot()
# box plot of Total Cup Points vs. Category Two Defects
data_sub %>%
ggplot(aes(Total.Cup.Points, Category.Two.Defects, group = Category.Two.Defects)) +
geom_box()
# box plot of Total Cup Points vs. Category Two Defects
data_sub %>%
ggplot(aes(Total.Cup.Points, Category.Two.Defects, group = Category.Two.Defects)) +
geom_boxplot()
# box plot of Total Cup Points vs. Quakers
data_sub %>%
ggplot(aes(Total.Cup.Points, Quakers, group = Quakers)) +
geom_boxplot()
# scatter plot of Total Cup Points vs. Mean Elevation
data_sub[is.na(data_sub$altitude_mean_meters)==0] %>%
ggplot(aes(Total.Cup.Points, altitude_mean_meters)) +
geom_point(alpha = 0.5)
data_sub$altitude_mean_meters
is.na(data_sub$altitude_mean_meters)==0
data_sub[is.na(data_sub$altitude_mean_meters)==0]
# scatter plot of Total Cup Points vs. Mean Elevation
data_sub[is.na(data_sub$altitude_mean_meters)==0,] %>%
ggplot(aes(Total.Cup.Points, altitude_mean_meters)) +
geom_point(alpha = 0.5)
data_sub[is.na(data_sub$altitude_mean_meters)==0 & data_sub$altitude_mean_meters < 5000,] %>%
ggplot(aes(Total.Cup.Points, altitude_mean_meters)) +
geom_point(alpha = 0.5)
